## This is a UNIX conf file that contains all information relating to
# the HAFS configuration.
# The syntax:
#
#      [section]
#      var = value
#
## Sets basic configuration options used by all components.
# This section sets basic configuration options used by all components.
[config]
## The main configuration file.
CONFhafs={com}/{stormlabel}.conf
## The holdvars file.
HOLDVARS={com}/{stormlabel}.holdvars.txt
# RUNhafs is a component of some other file paths
RUNhafs={SUBEXPT}

# Prefix to prepend to most output files in the COM directory.
out_prefix={vit[stormnamelc]}{vit[stnum]:02d}{vit[basin1lc]}.{vit[YMDH]}
out_prefix_nodate={vit[stormnamelc]}{vit[stnum]:02d}{vit[basin1lc]}
old_out_prefix={oldvit[stormnamelc]}{oldvit[stnum]:02d}{oldvit[basin1lc]}.{oldvit[YMDH]}
old_out_prefix_nodate={oldvit[stormnamelc]}{oldvit[stnum]:02d}{oldvit[basin1lc]}

ENS=99                ;; The ensemble number (placeholder)

# Pull data from external sources to a staging area.
# Specifies a section (default: [hafsdata]) to use: hafsdata, wcoss_fcst_nco
input_catalog=fcst_{GFSVER}

## Configure file and directory paths
[dir]
HOMEhafs={CDSAVE}/{EXPT}
WORKhafs={CDSCRUB}/{RUNhafs}/{vit[YMDH]}/{vit[stormid3]}
COMhafs={CDSCRUB}/{RUNhafs}/com/{vit[YMDH]}/{vit[stormid3]}
COMgfs={hafsdata/inputroot}
com={CDSCRUB}/{RUNhafs}/com/{vit[YMDH]}/{vit[stormid3]}
realstormcom={CDSCRUB}/{RUNhafs}/com/{YMDH}/{realstorm}
realstormwork={CDSCRUB}/{RUNhafs}/{YMDH}/{realstorm}
oldsid={oldvit[stormid3]}
oldcom={CDSCRUB}/{RUNhafs}/com/{oldvit[YMDH]}/{oldvit[stormid3]}
intercom={WORKhafs}/intercom                ;; dir for communicating data files between jobs
outatcf={CDNOSCRUB}/{SUBEXPT}               ;; Delivery location for ATCF files
outdiag={CDNOSCRUB}/diagtrak/{SUBEXPT}      ;; Delivery location for wrfdiag files
outstatus={CDNOSCRUB}/cycstatus/{SUBEXPT}   ;; Delivery location for status files
outatcfcorrected={CDNOSCRUB}/atcf/{SUBEXPT} ;; delivery location for corrected ATCF files
outships={CDNOSCRUB}/ships/{SUBEXPT}        ;; delivery location for SHIPS files
statusfile={WORKhafs}/{stormlabel}.{YMDH}   ;; cycle status file
## Domain center location file in COM.
domlocfile={com}/{vit[stnum]:02d}{vit[basin1lc]}.{vit[YMDH]}.domain.center
## File to check in a prior cycle's com, to see if the cycle exists.
HISTCHECK={oldcom}/{oldvit[stnum]:02d}{oldvit[basin1lc]}.{oldvit[YMDH]}.domain.center
## The name of the gsi status file in the com directory
gsistatus={stormlabel}.gsi_status
## Operational name of the gsi status file
gsistatus2=gsi_status.{vit[stormname]}{vit[stnum]:02d}{vit[basin1lc]}.{cycle}

PARMforecast={PARMhafs}/forecast/regional   ;; The location where the forecast job will find its parm and namelist files
PARMgsi={PARMhafs}/hafs-gsi/                ;; GSI input data for everything except CRTM
FIXcrtm={FIXhafs}/hafs-crtm-2.2.3/          ;; GSI CRTM input data

utilexec={HOMEhafs}/exec                    ;; utility exe location (placeholder)

# Data model locations
DOCNdir=/work/noaa/{disk_project}/{ENV[USER]}/DOCN
DATMdir=/work/noaa/{disk_project}/{ENV[USER]}/DATM

## Executable program locations
# Currently not used in the workflow script system
[exe]
tar=tar    ;; GNU Tar
htar=htar  ;; HTAR tape archiving program
hsi=hsi    ;; hsi tape manipulation program

wgrib2={ENV[WGRIB2]}
cnvgrib={ENV[CNVGRIB]}
wgrib={ENV[WGRIB]}
grbindex={ENV[GRIBINDEX]}
copygb2={ENV[COPYGB2]}
grb2index={ENV[GRB2INDEX]}

# hafs_forecast
forecast={EXEChafs}/hafs_forecast.x

# hafs_post
post={EXEChafs}/hafs_post.x

# hafs_utils
chgres={EXEChafs}/hafs_chgres.x
orog={EXEChafs}/hafs_orog.x
make_hgrid={EXEChafs}/hafs_make_hgrid.x
make_solo_mosaic={EXEChafs}/hafs_make_solo_mosaic.x
fregrid={EXEChafs}/hafs_fregrid.x
filter_topo={EXEChafs}/hafs_filter_topo.x
shave={EXEChafs}/hafs_shave.x

# hafs_tools
mpiserial={EXEChafs}/hafs_mpiserial.x ;; Executes serial programs via MPI
MPISERIAL={EXEChafs}/hafs_mpiserial.x ;; Executes serial programs via MPI

# hafs_vortextracker
gettrk={EXEChafs}/hafs_gettrk.x
tave={EXEChafs}/hafs_tave.x
vint={EXEChafs}/hafs_vint.x
supvit={EXEChafs}/hafs_supvit.x

# hafs_gsi
gsi={EXEChafs}/hafs_gsi.x
enkf={EXEChafs}/hafs_enkf.x

[prelaunch]
# Per-forecast-center configurations
rsmc_overrides=no      ;; read parm/hafs_JTWC.conf and parm/hafs_NHC.conf
rsmc_conf={PARMhafs}/hafs_{RSMC}.conf  ;; File to read for rsmc_overrides
# Per-basin configurations: read no_basin_conf if basin_conf is missing
basin_overrides=yes    ;; read parm/hafs_(basin).conf
# File to read for recognized basins when basin_overrides is enabled
basin_conf={PARMhafs}/hafs_{vit.pubbasin2}.conf
# File to read for unrecognized basins when basin_overrides is enabled
no_basin_conf={PARMhafs}/hafs_other_basins.conf

[launch]

# Sanity check options for the launch job
[sanity]

[grid]
CASE=C768            ;; FV3 resolution
LEVS=65              ;; Model vertical levels: 65
gtype=regional       ;; grid type: uniform, stretch, nest, or regional
# If gridfixdir is provided and the dir exists, then the hafs_grid job will
# just copy over the pre-generated static grid fix files under the gridfixdir.
#gridfixdir={FIXhafs}/fix_fv3/{CASE}
gridfixdir=/let/hafs_grid/generate/grid
# Otherwise, hafs_grid will generate the model grid according to the following grid parameters
# Need for grid types: stretch, nest and regional
stretch_fac=1.0001    ;; Stretching factor for the grid
# Use domlon and domlat if they are specified in the config session, otherwise
# domlon and domlat will be automatically generated according to the storm
# information
target_lon={domlon}  ;; center longitude of the highest resolution tile
target_lat={domlat}  ;; center latitude of the highest resolution tile
# Need for grid types: nest and regional
# The following options set a 2560x2160 regional grid with a refinement ratio of 4, sitting at the center of the tile
refine_ratio=4       ;; specify the refinement ratio for nest grid
istart_nest=128      ;; start index of the regional/nested domain on the tile's super grid
jstart_nest=228
iend_nest=1407       ;; end index of the regional/nested domain on the tile's super grid
jend_nest=1307
halo=3               ;; halo size to be used in the atmosphere cubic sphere model for the grid tile.
halop1=4             ;; halo size that will be used for the orography and grid tile in chgres
halo0=0              ;; no halo, used to shave the filtered orography for use in the model

[grid_ens]
CASE_ENS={grid/CASE}                  ;; FV3 resolution
LEVS_ENS={grid/LEVS}                  ;; Model vertical levels: 65
gtype_ens={grid/gtype}                ;; grid type: uniform, stretch, nest, or regional
gridfixdir_ens={grid/gridfixdir}
stretch_fac_ens={grid/stretch_fac}    ;; Stretching factor for the grid
target_lon_ens={grid/target_lon}      ;; center longitude of the highest resolution tile
target_lat_ens={grid/target_lat}      ;; center latitude of the highest resolution tile
refine_ratio_ens={grid/refine_ratio}  ;; Specify the refinement ratio for nest grid
istart_nest_ens={grid/istart_nest}
jstart_nest_ens={grid/jstart_nest}
iend_nest_ens={grid/iend_nest}
jend_nest_ens={grid/jend_nest}

[input]

[atm_ic]

[atm_lbc]

[chgres]

[vortexinit]

[bufrprep]

[fgat]

[gsi_vr]

[gsi]
use_bufr_nr=no            ;; use non-restricted version of observational bufr files
grid_ratio_fv3_regional=1 ;; ratio of analysis grid to fv3 model grid in fv3 grid units

[enkf]

[merge]

[forecast]
# ccpp suites
ccpp_suite_regional=HAFS_v0_gfdlmp_tedmf
ccpp_suite_glob=HAFS_v0_gfdlmp_tedmf
ccpp_suite_nest=HAFS_v0_gfdlmp_tedmf

#radiation scheme calling time steps
fhswr=1800.
fhlwr=1800.

# nsst related namelist options
# Choose nstf_name=2,0,0,0,0 when nemsio type ictype
# Choose nstf_name=2,1,0,0,0 when grib2 type ictype
nstf_n1=2
nstf_n2=0
nstf_n3=0
nstf_n4=0
nstf_n5=0

# Some options for FV3 model_configure
dt_atmos=90          ;; FV3 time step
restart_interval=0   ;; FV3 restart interval in hours
# For the global domain if it exists in the model configuration
glob_k_split=1
glob_n_split=7
glob_layoutx=8
glob_layouty=8
glob_npx=769
glob_npy=769
# For the nested or regional standalone domain
k_split=4
n_split=5
layoutx=40
layouty=30
npx=2561
npy=2161
npz=64

# The write_grid_component related options
quilting=.true.
write_groups=2
write_tasks_per_group=48
write_dopost=.true.
output_history=.false.
app_domain=regional              ;; write_grid_component output domain: regional, nest, or global

# The option for output grid type: rotated_latlon, regional_latlon
# Currently, the cubed_sphere_grid option is only supported by the forecast job, the post and product jobs cannot work for cubed_sphere_grid yet.
#output_grid='cubed_sphere_grid'
#
output_grid=rotated_latlon
output_grid_cen_lon={domlon}      ;; central longitude
output_grid_cen_lat={domlat}      ;; central latitude
output_grid_lon_span=78.0         ;; output domain span for longitude in rotated coordinate system (in degrees)
output_grid_lat_span=69.0         ;; output domain span for latitude in rotated coordinate system (in degrees)
output_grid_dlon=0.03             ;; output grid spacing dlon . . . .
output_grid_dlat=0.03             ;; output grid spacing dlat . . . .
# If the following options are not set, their values will be calculated according to cen_lon(lat), lon(lat)_span
#output_grid_lon1=-39.0            ;; longitude of lower-left . . . .
#output_grid_lat1=-34.5            ;; latitude of lower-left . . . .
#output_grid_lon2=39.0             ;; longitude of upper-right . . . .
#output_grid_lat2=34.5             ;; latitude of upper-right . . . .

#output_grid=regional_latlon
#output_grid_cen_lon={domlon}      ;; central longitude
#output_grid_cen_lat={domlat}      ;; central latitude
#output_grid_lon_span=96.0         ;; output domain span for longitude in regular latlon coordinate system (in degrees)
#output_grid_lat_span=69.0         ;; output domain span for latitude in regular latlon coordinate system (in degrees)
#output_grid_dlon=0.03             ;; output grid spacing dlon . . . .
#output_grid_dlat=0.03             ;; output grid spacing dlat . . . .

# Ocean coupling related options. Only useful when run_ocean=yes
# When run_ocean=yes, then the following options are available for cpl_ocean
# 0, run ocean model side by side (no coupling)
# 1, direct coupling through the nearest point regridding method
# 2, direct coupling through the bilinear regridding method
# 3, CMEPS based coupling through the bilinear regridding method
cpl_ocean=0
cpl_dt=360  ;; coupling time step in seconds
#ocean_tasks=120 ;; Number of PEs for the OCN component
ocean_tasks=60 ;; Number of PEs for the OCN component
ocean_start_dtg=auto ;; epoch day since hycom_epoch=datetime.datetime(1900,12,31,0,0,0), e.g., 43340.00000
merge_import=.true.

# Data model defaults
merged_docn_input={WORKhafs}/intercom/oisst-avhrr-v02r01.{YMD}.nc
mesh_ocn_in={DOCNdir}/mesh/TX025_201215_ESMFmesh_direct.nc ;; premade mesh to use if make_mesh_ocn=no
mesh_ocn_gen={WORKhafs}/intercom/DOCN_ESMF_mesh.nc ;; do not change
mesh_atm_in=/work/noaa/{disk_project}/{ENV[USER]}/static/TL639_200618_ESMFmesh.nc ;; premade mesh to use if make_mesh_atm=no
mesh_atm_gen={WORKhafs}/intercom/DATM_ESMF_mesh.nc ;; do not change

[forecast_ens]
# ccpp suites
ccpp_suite_regional_ens={forecast/ccpp_suite_regional}
ccpp_suite_glob_ens={forecast/ccpp_suite_glob}
ccpp_suite_nest_ens={forecast/ccpp_suite_nest}

# Some options for FV3 model_configure
dt_atmos_ens={forecast/dt_atmos}
restart_interval_ens={forecast/restart_interval}
# For the global domain if it exists in the model configuration
glob_k_split_ens={forecast/glob_k_split}
glob_n_split_ens={forecast/glob_n_split}
glob_layoutx_ens={forecast/glob_layoutx}
glob_layouty_ens={forecast/glob_layouty}
glob_npx_ens={forecast/glob_npx}
glob_npy_ens={forecast/glob_npy}
# For the nested or regional standalone domain
k_split_ens={forecast/k_split}
n_split_ens={forecast/n_split}
layoutx_ens={forecast/layoutx}
layouty_ens={forecast/layouty}
npx_ens={forecast/npx}
npy_ens={forecast/npy}
npz_ens={forecast/npz}

# The write_grid_component related options
quilting_ens={forecast/quilting}
write_groups_ens={forecast/write_groups}
write_tasks_per_group_ens={forecast/write_tasks_per_group}
write_dopost_ens={forecast/write_dopost}
output_history_ens={forecast/output_history}

# Placeholders currently, not implemented yet
output_grid_ens={forecast/output_grid_ens}
output_grid_cen_lon_ens={forecast/output_grid_cen_lon_ens}
output_grid_cen_lat_ens={forecast/output_grid_cen_lat_ens}
output_grid_lon_span_ens={forecast/output_grid_lon_span_ens}
output_grid_lat_span_ens={forecast/output_grid_lat_span_ens}
output_grid_dlon_ens={forecast/output_grid_dlon_ens}
output_grid_dlat_ens={forecast/output_grid_dlat_ens}

[atm_post]
# Grid definition for atm_post and tracker, used by wgrib2
# Example:
#   synop_gridspecs="latlon 246.6:4112:0.025 -2.4:1976:0.025"
# latlon lon0:nlon:dlon lat0:nlat:dlat
# lat0, lon0 = degrees of lat/lon for 1st grid point
# nlon = number of longitudes
# nlat = number of latitudes
# dlon = grid cell size in degrees of longitude
# dlat = grid cell size in degrees of latitude
#
# if synop_gridspecs=auto, which is the default, then synop_gridspecs will be automatically generated based on the output grid
# if output_grid is rotated_latlon
# lon0=output_grid_cen_lon+output_grid_lon1-9.
# lat0=output_grid_cen_lat+output_grid_lat1
# dlon=output_grid_dlon
# dlat=output_grid_dlat
# nlon=(output_grid_lon2-output_grid_lon1+18.)/output_grid_dlon
# nlat=(output_grid_lat2-output_grid_lat2)/output_grid_dlat
# if output_grid is regional_latlon, synop_gridspecs will be the same as the ouput regular latlon grid
#
#synop_gridspecs="latlon 246.6:4112:0.025 -2.4:1976:0.025"
synop_gridspecs=auto
trker_gridspecs={synop_gridspecs} ;; Currently a placeholder, and the traker uses the same grid as the output grid

[atm_post_ens]
synop_gridspecs_ens=auto
trker_gridspecs_ens={synop_gridspecs_ens} ;; Currently a placeholder, and the traker uses the same grid as the output grid

[product]

[tracker]

[archive]
mkdir=yes     ;; make the archive directory? yes or no
# To turn on archiving fv3 output netcdf files
#fv3out=hpss:/NCEPDEV/{tape_project}/2year/{ENV[USER]}/{SUBEXPT}/fv3out/{out_prefix}.tar

## Variables to set as string values when parsing the hafs_workflow.xml.in.
# This section is only used by the rocoto-based workflow
[rocotostr]
CDSAVE={dir/CDSAVE}                 ;; save area for Rocoto to use
CDNOSCRUB={dir/CDNOSCRUB}           ;; non-scrubbed area for Rocoto to use
CDSCRUB={dir/CDSCRUB}               ;; scrubbed area for Rocoto to use
PARMhafs={dir/PARMhafs}             ;; parm/ directory location
USHhafs={dir/USHhafs}               ;; ush/ directory location
EXhafs={dir/EXhafs}                 ;; scripts/ directory location
JOBhafs={dir/JOBhafs}               ;; scripts/ directory location
EXPT={config/EXPT}                  ;; experiment name
SUBEXPT={config/SUBEXPT}            ;; sub-experiment name
CPU_ACCOUNT={cpu_account}           ;; CPU account name
COMgfs={dir/COMgfs}                 ;; input GFS com directory
COMrtofs={dir/COMrtofs}             ;; input RTOFS com directory
gtype={grid/gtype}                  ;; grid type: uniform, stretch, nest, or regional (currently only nest and regional have been tested and supported)
GFSVER={config/GFSVER}              ;; Version of GFS input data, e.g., PROD2019, PROD2021
DOWNLOAD_DATM_LIST={download_datm_list} ;; List of task ids for era5 downloader
DOWNLOAD_DATM_THRESHOLD={download_datm_threshold} ;; 1.0/(length of download_datm_list)
DOWNLOAD_DOCN_LIST={download_docn_list} ;; List of task ids for era5 downloader
DOWNLOAD_DOCN_THRESHOLD={download_docn_threshold} ;; 1.0/(length of download_docn_list)

# Specify the forecast job resources. Only a few combinations are provided. If
# needed, you may add other options in the site entity files under rocoto/sites.
#FORECAST_RESOURCES=FORECAST_RESOURCES_regional_{forecast/layoutx}x{forecast/layouty}io{forecast/write_groups}x{forecast/write_tasks_per_group}_omp2

# Variables to set as boolean values when parsing the hafs_workflow.xml.in.
# They'll be changed to YES or NO.  This section is only used by the rocoto-based workflow.
[rocotobool]
RUN_GSI_VR={run_gsi_vr}             ;; Do we run GSI based vortex relocation?
RUN_GSI_VR_FGAT={run_gsi_vr_fgat}   ;; Do we run GSI based vortex relocation for FGAT?
RUN_GSI_VR_ENS={run_gsi_vr_ens}     ;; Do we run GSI based vortex relocation for ensda members?
RUN_GSI={run_gsi}                   ;; Do we run GSI?
RUN_FGAT={run_fgat}                 ;; Do we run DA with FGAT?
RUN_ENVAR={run_envar}               ;; Do we run hybrid EnVar with either GDAS ensembles or regional ensembles?
RUN_ENSDA={run_ensda}               ;; Do we run the ensda system?
RUN_ENKF={run_enkf}                 ;; Do we run the self-cycled ensda system with EnKF analysis
RUN_OCEAN={run_ocean}               ;; Do we run with ocean coupling?
RUN_WAVE={run_wave}                 ;; Do we run with wave coupling?
RUN_DATM={run_datm}                 ;; Do we run with a data atmosphere using CDEPS?
RUN_DOCN={run_docn}                 ;; Do we run with a data ocean using CDEPS?
RUN_DWAV={run_dwav}                 ;; Do we run with data waves using CDEPS?
RUN_VORTEXINIT={run_vortexinit}     ;; Do we enable vortex initialization?
RUN_HRDGRAPHICS={run_hrdgraphics}   ;; Do we run HRD graphics?
RUN_EMCGRAPHICS={run_emcgraphics}   ;; Do we run EMC graphics?
SCRUB_COM={scrub_com}               ;; Should Rocoto scrub the COM directory?
SCRUB_WORK={scrub_work}             ;; Should Rocoto scrub the WORK directory?
MAKE_MESH_ATM={make_mesh_atm}       ;; Should the DATM mesh be generated by the workflow?
DOWNLOAD_DATM={download_datm}       ;; Should the workflow download DATM ERA5 files?
MAKE_MESH_OCN={make_mesh_ocn}       ;; Should the DOCN mesh be generated by the workflow?
DOWNLOAD_DOCN={download_docn}       ;; Should the workflow download DOCN OISST files?
